import os
import threading
import concurrent.futures
import logging
import json
from typing import Dict, Any
from ai_prompts import GroqClient
from utils_cv import extract_text_from_file
from openings_db_manager import load_openings_db
from database import AnalysisDatabase

# ---------- CONFIGURA칂츾O ----------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configura칞칫es globais
MAX_WORKERS = 4
OUTPUT_DIR = "analises_cv"
GROQ_CLIENT = GroqClient()

# Cria uma 칰nica inst칙ncia do banco de dados no escopo global
database = AnalysisDatabase(db_path='applicants.json')
# Lock para garantir que a escrita no console n칚o se misture
console_lock = threading.Lock()

# ---------- FUN칂츾O DE PROCESSAMENTO ----------
def process_single_cv(cv_path: str, opening_data: Dict[str, Any]):
    """Processa um 칰nico CV e gera a an치lise de alinhamento."""
    
    # 游 L칩gica de checagem de duplicidade:
    # Cria o nome do arquivo de an치lise que seria gerado
    candidate_name = os.path.basename(cv_path).split('.')[0]
    safe_name = "".join(c for c in candidate_name if c.isalnum() or c in (' ', '.', '_')).rstrip()
    safe_opening_title = "".join(c for c in opening_data.get('title', 'vaga_desconhecida') if c.isalnum() or c in (' ', '_')).rstrip().replace(' ', '_')
    output_folder = os.path.join(OUTPUT_DIR, opening_data.get('folder', 'outros'))
    output_file = os.path.join(output_folder, f"{safe_name}_{safe_opening_title}.md")

    # Verifica se o arquivo de an치lise j치 existe
    if os.path.exists(output_file):
        with console_lock:
            logger.info(f"An치lise para '{candidate_name}' na vaga '{opening_data.get('title')}' j치 existe. Pulando.")
        return
    
    # 游 Fim da checagem de duplicidade 游

    MAX_RETRIES = 5
    retries = 0
    full_analysis = None
    
    try:
        with console_lock:
            logger.info(f"--- Processando CV: {os.path.basename(cv_path)} para a vaga '{opening_data.get('title', 'N/A')}' (ID: {opening_data.get('id', 'N/A')}) ---")

        cv_text = extract_text_from_file(cv_path)
        
        if not cv_text or len(cv_text.split()) < 50:
            with console_lock:
                logger.error(f"Falha na extra칞칚o de texto do CV {os.path.basename(cv_path)} ou conte칰do muito curto. Pulando.")
            return

        cleaned_cv_text = ' '.join(cv_text.split()[:4000])
        
    except Exception as e:
        with console_lock:
            logger.error(f"Erro ao extrair texto do CV {os.path.basename(cv_path)}: {e}")
        return

    # L칩gica de retentativa para a chamada da API
    while retries < MAX_RETRIES:
        try:
            job_description = (
                opening_data.get('intro', '') + ' ' + 
                opening_data.get('main_activities', '') + ' ' + 
                opening_data.get('add_infos', '') + ' ' +
                opening_data.get('pre_requisites', '')
            ).strip()

            # A sua chamada  API n칚o tinha o `opening_json`
            full_analysis = GROQ_CLIENT.generate_full_cv_analysis(cleaned_cv_text, job_description)
            
            if full_analysis and 'conclusion' in full_analysis and 'score' in full_analysis:
                break 
            
            with console_lock:
                logger.warning(f"Resposta incompleta da API para {os.path.basename(cv_path)}. Tentando novamente ({retries + 1}/{MAX_RETRIES}).")
            retries += 1
            
        except Exception as e:
            with console_lock:
                logger.error(f"Erro na requisi칞칚o para {os.path.basename(cv_path)}: {e}")
            retries += 1

    if not full_analysis or retries >= MAX_RETRIES:
        with console_lock:
            logger.error(f"Falha ao analisar o CV {os.path.basename(cv_path)} ap칩s {MAX_RETRIES} tentativas. Pulando.")
        return

    # Extrai os dados da an치lise final
    conclusion = full_analysis.get('conclusion', 'Conclus칚o n칚o gerada.')
    score = full_analysis.get('score', 0.0)
    structured_data = full_analysis.get('structured_data', {})
    total_experience_years = full_analysis.get('total_experience_years', 'N칚o avaliado')
    
    # Adiciona a an치lise ao banco de dados - L칩gica movida para dentro da fun칞칚o
    opening_id = opening_data.get("id")
    brief_data = {
        'cv_text': cv_text,
        'cv_path': cv_path,
        'content': conclusion
    }
    brief_id = database.add_brief_data(brief_data=brief_data, file_path=cv_path)

    analysis_to_save = {
        "name": structured_data.get('name'),
        "formal_education": structured_data.get('formal_education'),
        "hard_skills": structured_data.get('hard_skills'),
        "soft_skills": structured_data.get('soft_skills'),
        "score": score,
        "total_experience_years": total_experience_years
    }
    
    database.add_analysis_data(opening_id, brief_id, analysis_to_save)
    with console_lock:
        logger.info(f"An치lise de {structured_data.get('name')} salva no banco de dados para a vaga '{opening_data.get('title')}'")

    # Cria칞칚o do diret칩rio e escrita do arquivo .md
    os.makedirs(output_folder, exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("# An치lise do Curr칤culo\n\n")
        f.write(f"## {candidate_name}\n")
        f.write(f"**Vaga:** {opening_data.get('title', 'N/A')}\n") 
        f.write(f"**Pontua칞칚o:** {score:.2f}/10\n")
        f.write(f"**Tempo de Experi칡ncia:** {total_experience_years} anos\n\n")
        f.write("---\n\n")
        f.write("## Resumo do Candidato\n")
        
        f.write("### Nome Completo\n")
        f.write(f"{structured_data.get('name', 'Nenhuma informa칞칚o dispon칤vel')}\n\n")
        f.write("### Habilidades T칠cnicas\n")
        f.write(", ".join(structured_data.get('hard_skills', [])))
        f.write("\n\n### Habilidades Comportamentais\n")
        f.write(", ".join(structured_data.get('soft_skills', [])))
        f.write("\n\n### Forma칞칚o Principal\n")
        f.write(f"{structured_data.get('formal_education', 'Nenhuma informa칞칚o dispon칤vel')}\n\n")
        
        f.write("---\n\n")
        f.write("## Conclus칚o\n")
        f.write(conclusion)
        
    with console_lock:
        logger.info(f"An치lise de {candidate_name} para a vaga '{opening_data.get('title', 'N/A')}' salva em {output_file}")

# ---------- FUN칂츾O PRINCIPAL ----------
def main():
    """Fun칞칚o principal para processar todos os curr칤culos para todas as vagas."""
    
    cv_base_dir = "banco-de-talentos"
    job_openings = load_openings_db()

    if not job_openings:
        logger.error("Nenhuma vaga encontrada para processamento. Verifique o arquivo 'openings_db.json'.")
        return

    # Mapeia as pastas para as vagas para um loop mais eficiente
    folder_to_opening = {data['folder']: data for data in job_openings.values()}

    all_tasks = []
    
    # Itera sobre as pastas de curr칤culos
    for folder_name in os.listdir(cv_base_dir):
        folder_path = os.path.join(cv_base_dir, folder_name)

        if not os.path.isdir(folder_path) or folder_name not in folder_to_opening:
            continue

        opening_data = folder_to_opening[folder_name]
        cv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(('.pdf', '.docx'))]
        
        if not cv_files:
            logger.warning(f"Nenhum curr칤culo encontrado no diret칩rio '{folder_path}'.")
            continue

        logger.info(f"## Processando {len(cv_files)} curr칤culos para a vaga '{opening_data['title']}' (Pasta: {folder_name}). ##")
        
        for cv_file in cv_files:
            all_tasks.append((cv_file, opening_data))

    if not all_tasks:
        logger.info("Nenhum curr칤culo para processar. Finalizando.")
        return

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_single_cv, cv_file, opening_data) for cv_file, opening_data in all_tasks]
        
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                logger.error(f"Uma das tarefas de processamento falhou: {e}")
    
    logger.info("## Processamento de todos os curr칤culos conclu칤do. ##\n")

if __name__ == "__main__":
    main()